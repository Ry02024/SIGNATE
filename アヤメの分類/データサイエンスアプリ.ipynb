{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3IFWjkXxwy+/mx9x3U4nB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ry02024/SIGNATE/blob/main/%E3%82%A2%E3%83%A4%E3%83%A1%E3%81%AE%E5%88%86%E9%A1%9E/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9%E3%82%A2%E3%83%97%E3%83%AA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#streamlit：簡単な分析と機械学習"
      ],
      "metadata": {
        "id": "l9LbUZMA1rxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##環境構築"
      ],
      "metadata": {
        "id": "d_4FA9X01rxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q"
      ],
      "metadata": {
        "id": "-0PRTv841rxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a2b1872-d8e9-42b5-acb3-14bb3a591d53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "yXfzNyhE1rxG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')"
      ],
      "metadata": {
        "id": "wTBjGPdv1rxG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# ngrokトークンを設定\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0070482-869f-46ae-d6c3-1daee5ecfe1a",
        "id": "SUP7mo2B1rxG"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##実行ファイル"
      ],
      "metadata": {
        "id": "KfkVixYM1rxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###version1"
      ],
      "metadata": {
        "id": "sMerQbWVeyRE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install japanize-matplotlib -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBdlaDmAAhZs",
        "outputId": "233856e8-b7ef-4049-85b1-0926ee6b7440"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import japanize_matplotlib\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# --- 関数 ---\n",
        "def load_data(file, file_type=\"訓練\"):\n",
        "    \"\"\"\n",
        "    ファイルをアップロードし、データフレームを読み込む。\n",
        "    \"\"\"\n",
        "    if file is not None:\n",
        "        data = pd.read_csv(file, delimiter='\\t')\n",
        "        if 'id' in data.columns:\n",
        "            data.drop(columns='id', inplace=True)\n",
        "        st.write(f\"{file_type}データの最初の5行:\")\n",
        "        st.write(data.head())\n",
        "        return data\n",
        "    return None\n",
        "\n",
        "def visualize_data_analysis(data):\n",
        "    \"\"\"\n",
        "    データ分析の可視化を実行する。\n",
        "    \"\"\"\n",
        "    if data is not None:\n",
        "        st.write('基本統計量:')\n",
        "        st.write(data.describe())\n",
        "\n",
        "        st.write('特徴量のヒストグラム:')\n",
        "        fig, ax = plt.subplots(figsize=(15, 10))\n",
        "        data.hist(bins=15, ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.write('散布図とカーネル密度推定:')\n",
        "        pair_plot_fig = sns.pairplot(data, hue='class', markers=[\"o\", \"s\", \"D\"], palette=\"bright\")\n",
        "        st.pyplot(pair_plot_fig)\n",
        "\n",
        "        st.write('相関行列:')\n",
        "        numeric_data = data.select_dtypes(include=['number'])\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm', ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.session_state['visualization_shown'] = True  # 可視化が実行されたことを記録\n",
        "\n",
        "def train_and_evaluate_model(data):\n",
        "    \"\"\"\n",
        "    モデルを訓練し、評価指標を表示する。\n",
        "    \"\"\"\n",
        "    global best_model, scaler # best_modelとscalerをグローバル変数として宣言\n",
        "    if data is not None:\n",
        "        X_train = data.drop('class', axis=1)\n",
        "        y_train = data['class']\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "        param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "        grid_search.fit(X_train_scaled, y_train)\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        y_train_pred = best_model.predict(X_train_scaled)\n",
        "        st.write(\"訓練データの精度: \", accuracy_score(y_train, y_train_pred))\n",
        "        st.write(\"適合率:\", precision_score(y_train, y_train_pred, average='macro'))\n",
        "        st.write(\"再現率:\", recall_score(y_train, y_train_pred, average='macro'))\n",
        "        st.write(\"F1スコア:\", f1_score(y_train, y_train_pred, average='macro'))\n",
        "\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.heatmap(confusion_matrix(y_train, y_train_pred), annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "        plt.title(\"訓練データの混同行列\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "def predict_test_data(data):\n",
        "    \"\"\"\n",
        "    テストデータの予測を実行する。\n",
        "    \"\"\"\n",
        "    global best_model, scaler\n",
        "    if data is not None and best_model is not None:\n",
        "        X_test_scaled = scaler.transform(data)\n",
        "        y_test_pred = best_model.predict(X_test_scaled)\n",
        "        st.write(\"テストデータの予測結果:\")\n",
        "        st.write(pd.DataFrame({'Prediction': y_test_pred}))\n",
        "\n",
        "def update_model_with_additional_data(additional_data):\n",
        "    \"\"\"\n",
        "    追加データでモデルを更新する。\n",
        "    \"\"\"\n",
        "    global train_data, best_model\n",
        "    if additional_data is not None and train_data is not None:\n",
        "        train_data = pd.concat([train_data, additional_data], ignore_index=True)\n",
        "        st.write(\"統合後のデータの最初の5行:\")\n",
        "        st.write(train_data.head())\n",
        "        if best_model is not None:\n",
        "            train_and_evaluate_model(train_data)\n",
        "\n",
        "# --- main ---\n",
        "def main():\n",
        "    st.title('アヤメの品種分類アプリ')\n",
        "    st.write('このアプリはアヤメのデータセットを分析し、品種を予測します。')\n",
        "\n",
        "    # データフレームとモデルを格納する変数\n",
        "    global train_data, test_data, best_model, scaler\n",
        "\n",
        "    # ファイルアップローダー\n",
        "    train_file = st.file_uploader(\"訓練データをアップロードしてください（train.tsv）\", type=['tsv'])\n",
        "    test_file = st.file_uploader(\"テストデータをアップロードしてください（test.tsv）\", type=['tsv'])\n",
        "\n",
        "    train_data = load_data(train_file)\n",
        "    test_data = load_data(test_file, \"テスト\")\n",
        "\n",
        "    # セッションステートの初期化\n",
        "    if 'visualization_shown' not in st.session_state:\n",
        "        st.session_state['visualization_shown'] = False\n",
        "\n",
        "    # 可視化の実行\n",
        "    visualize = st.checkbox('データを可視化して分析')\n",
        "    if visualize and train_data is not None:\n",
        "        visualize_data_analysis(train_data)\n",
        "\n",
        "    # モデル訓練ボタン\n",
        "    train_model = st.checkbox('モデルを訓練して評価')\n",
        "    if train_model and train_data is not None:\n",
        "        train_and_evaluate_model(train_data)\n",
        "\n",
        "    # 予測ボタン\n",
        "    predict = st.checkbox('テストデータを予測')\n",
        "    if predict and test_data is not None:\n",
        "        predict_test_data(test_data)\n",
        "\n",
        "    # 追加データのアップロード\n",
        "    additional_data = load_data(st.file_uploader(\"追加のデータセットをアップロードして訓練データに追加\", type=['tsv']), \"追加\")\n",
        "    if additional_data is not None:\n",
        "        update_model_with_additional_data(additional_data)\n",
        "\n",
        "# アプリケーションの実行\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TT2ZDiyd-CCo",
        "outputId": "584a6bb7-e227-4203-f32c-5162e936d9a6"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##アプリの起動"
      ],
      "metadata": {
        "id": "W4iAda4i1rxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ngrokを介してStreamlitを公開\n",
        "public_url = ngrok.connect(addr='8501')\n",
        "print('Public URL:', public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "331ac10b-70a7-4184-fe80-9a2eec5cb0f1",
        "id": "17c_nK0C1rxH"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://bb24-34-136-117-121.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlitアプリケーションの起動\n",
        "!streamlit run streamlit_app.py >/dev/null"
      ],
      "metadata": {
        "id": "RL66sp521rxI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feb378b8-19c8-411f-f32e-35abeb4a7afe"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/streamlit_app.py:35: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
            "  data.hist(bins=15, ax=ax)\n",
            "/content/streamlit_app.py:35: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
            "  data.hist(bins=15, ax=ax)\n",
            "/content/streamlit_app.py:35: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
            "  data.hist(bins=15, ax=ax)\n",
            "/content/streamlit_app.py:35: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
            "  data.hist(bins=15, ax=ax)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VJt2fSkIBx_X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}