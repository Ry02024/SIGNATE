{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNA7s901IJ7V2ZZGWF/6+RA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ry02024/SIGNATE/blob/main/%E3%82%A2%E3%83%A4%E3%83%A1%E3%81%AE%E5%88%86%E9%A1%9E/%E3%83%87%E3%83%BC%E3%82%BF%E3%82%B5%E3%82%A4%E3%82%A8%E3%83%B3%E3%82%B9%E3%82%A2%E3%83%97%E3%83%AA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#streamlit：簡単な分析と機械学習"
      ],
      "metadata": {
        "id": "l9LbUZMA1rxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##環境構築"
      ],
      "metadata": {
        "id": "d_4FA9X01rxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q"
      ],
      "metadata": {
        "id": "-0PRTv841rxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c666b814-d62c-4f6d-8448-592a66ca440f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from pyngrok import ngrok"
      ],
      "metadata": {
        "id": "yXfzNyhE1rxG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')"
      ],
      "metadata": {
        "id": "wTBjGPdv1rxG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import os\n",
        "\n",
        "# ngrokトークンを設定\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d744376e-3730-4fcb-e1cb-4cc02d3c3ad5",
        "id": "SUP7mo2B1rxG"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##実行ファイル"
      ],
      "metadata": {
        "id": "KfkVixYM1rxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install japanize-matplotlib -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBdlaDmAAhZs",
        "outputId": "74d6644c-b040-45b8-fc4d-356172196bcc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for japanize-matplotlib (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###util.py"
      ],
      "metadata": {
        "id": "mMQqHDnmPaRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile util.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import japanize_matplotlib\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "import google.generativeai as genai\n",
        "\n",
        "def load_data(file, drop_id=True):\n",
        "    \"\"\"\n",
        "    ファイルをアップロードし、データフレームを読み込む。\n",
        "    \"\"\"\n",
        "    if file is not None:\n",
        "        data = pd.read_csv(file, delimiter='\\t')\n",
        "        if drop_id and 'id' in data.columns:\n",
        "            data.drop(columns='id', inplace=True)\n",
        "        return data\n",
        "    return None\n",
        "\n",
        "def visualize_data_analysis(data):\n",
        "    \"\"\"\n",
        "    データ分析の可視化を実行する。\n",
        "    \"\"\"\n",
        "    if data is not None:\n",
        "        st.write('基本統計量:')\n",
        "        st.write(data.describe())\n",
        "\n",
        "        st.write('特徴量のヒストグラム:')\n",
        "        fig, ax = plt.subplots()\n",
        "        data.hist(bins=15, figsize=(15, 10), ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        st.write('散布図とカーネル密度推定:')\n",
        "        pair_plot_fig = sns.pairplot(data, hue='class', markers=[\"o\", \"s\", \"D\"], palette=\"bright\")\n",
        "        st.pyplot(pair_plot_fig)\n",
        "\n",
        "        st.write('相関行列:')\n",
        "        numeric_data = data.select_dtypes(include=['number'])\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.heatmap(numeric_data.corr(), annot=True, cmap='coolwarm', ax=ax)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "def summarize_visualizations(data):\n",
        "    \"\"\"\n",
        "    データの可視化結果を要約する。\n",
        "    \"\"\"\n",
        "    summary = \"\"\n",
        "\n",
        "    # 基本統計量\n",
        "    summary += \"基本統計量:\\n\"\n",
        "    summary += data.describe().to_markdown() + \"\\n\\n\"\n",
        "\n",
        "    # ヒストグラム\n",
        "    summary += \"特徴量のヒストグラム:\\n\"\n",
        "    for column in data.columns:\n",
        "        if data[column].dtype in ['int64', 'float64']:\n",
        "            summary += f\"{column}の分布: 平均={data[column].mean()}, 標準偏差={data[column].std()}, 最大値={data[column].max()}, 最小値={data[column].min()}\\n\"\n",
        "\n",
        "    # 相関行列\n",
        "    numeric_data = data.select_dtypes(include=['number'])\n",
        "    corr_matrix = numeric_data.corr()\n",
        "    summary += \"\\n相関行列:\\n\"\n",
        "    summary += corr_matrix.to_markdown() + \"\\n\"\n",
        "\n",
        "    # 散布図\n",
        "    summary += \"\\n散布図:\\n\"\n",
        "    summary += \"各特徴量間の散布図は以下のように分布しています。\\n\"\n",
        "\n",
        "    return summary\n",
        "\n",
        "def generate_insights(data):\n",
        "    if data is not None:\n",
        "        insights = []\n",
        "        desc = data.describe()\n",
        "\n",
        "        # 基本統計量からの知見\n",
        "        insights.append(\"基本統計量から、各特徴量の平均値、中央値、標準偏差がわかります。これにより、特徴量の分布と中心傾向が理解できます。\")\n",
        "\n",
        "        # 相関行列からの知見\n",
        "        numeric_data = data.select_dtypes(include=['number'])\n",
        "        corr_matrix = numeric_data.corr()\n",
        "        high_corr = corr_matrix[(corr_matrix > 0.7) & (corr_matrix != 1.0)]\n",
        "        if not high_corr.empty:\n",
        "            insights.append(\"相関行列から、以下の特徴量間で強い相関が見られます:\")\n",
        "            insights.append(high_corr.dropna(how='all').dropna(axis=1, how='all').to_markdown())\n",
        "        else:\n",
        "            insights.append(\"相関行列から、特徴量間に強い相関は見られませんでした。\")\n",
        "\n",
        "        # クラスごとの分布からの知見\n",
        "        class_distribution = data['class'].value_counts()\n",
        "        insights.append(f\"クラスごとの分布は以下の通りです:\\n{class_distribution.to_markdown()}\\nこれにより、データセット内の各クラスの割合がわかります。\")\n",
        "\n",
        "        return insights\n",
        "    return []\n",
        "\n",
        "def train_and_evaluate_model(train_data, test_data):\n",
        "    \"\"\"\n",
        "    モデルを訓練し、評価指標を表示する。\n",
        "    \"\"\"\n",
        "    if train_data is not None and test_data is not None:\n",
        "        # 特徴量とターゲットに分割\n",
        "        X_train = train_data.drop('class', axis=1)\n",
        "        y_train = train_data['class']\n",
        "        X_test = test_data\n",
        "\n",
        "        # 特徴量のスケーリング\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "        # モデル構築とハイパーパラメータのチューニング\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "        param_grid = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20, 30]}\n",
        "        grid_search = GridSearchCV(model, param_grid, cv=5)\n",
        "        grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "        # 最適なモデルを取得\n",
        "        best_model = grid_search.best_estimator_\n",
        "\n",
        "        # 訓練データに対する予測\n",
        "        y_train_pred = best_model.predict(X_train_scaled)\n",
        "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
        "        train_confusion = confusion_matrix(y_train, y_train_pred)\n",
        "\n",
        "        # 適合率、再現率、F1スコアを計算\n",
        "        precision = precision_score(y_train, y_train_pred, average='macro')\n",
        "        recall = recall_score(y_train, y_train_pred, average='macro')\n",
        "        f1 = f1_score(y_train, y_train_pred, average='macro')\n",
        "\n",
        "        st.write(\"訓練データの精度: \", train_accuracy)\n",
        "        st.write(\"適合率:\", precision)\n",
        "        st.write(\"再現率:\", recall)\n",
        "        st.write(\"F1スコア:\", f1)\n",
        "        fig, ax = plt.subplots()\n",
        "        sns.heatmap(train_confusion, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "        plt.title(\"訓練データの混同行列\")\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # テストデータに対する予測\n",
        "        y_test_pred = best_model.predict(X_test_scaled)\n",
        "        st.write(\"テストデータの予測結果:\")\n",
        "        st.write(pd.DataFrame({'Prediction': y_test_pred}))\n",
        "\n",
        "        # 予測結果の要約を生成\n",
        "        prediction_summary = f\"\"\"\n",
        "        訓練データの精度: {train_accuracy}\n",
        "        適合率: {precision}\n",
        "        再現率: {recall}\n",
        "        F1スコア: {f1}\n",
        "        \"\"\"\n",
        "        return prediction_summary, y_test_pred\n",
        "    return None, None\n",
        "\n",
        "def get_visualization_insights(visualization_summary):\n",
        "    \"\"\"\n",
        "    データ分析の可視化結果に基づいた知見を取得する。\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    combined_text = f\"\"\"\n",
        "    以下のデータ分析結果に基づいて、データの詳細な分析を提供してください。\n",
        "\n",
        "    **データの可視化結果の要約:**\n",
        "    {visualization_summary}\n",
        "    \"\"\"\n",
        "    response = model.generate_content(combined_text)\n",
        "    return response.text\n",
        "\n",
        "def get_insights_from_gemini(summary, y_test_pred):\n",
        "    \"\"\"\n",
        "    Gemini APIを使用して、予測結果に基づいた知見を取得する。\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel('gemini-pro')\n",
        "    combined_text = f\"\"\"\n",
        "    以下の予測結果に基づいて、モデルのパフォーマンスと結果についての詳細な分析を提供してください。\n",
        "\n",
        "    **予測結果の要約:**\n",
        "    {summary}\n",
        "\n",
        "    **テストデータの予測結果:**\n",
        "    {y_test_pred}\n",
        "    \"\"\"\n",
        "    response = model.generate_content(combined_text)\n",
        "    return response.text\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5haNPB5k1Z2",
        "outputId": "77498330-9431-4d4b-823b-2ad5088d91cd"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting util.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###streamlit_app.py"
      ],
      "metadata": {
        "id": "7wQMBbtEkUb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile streamlit_app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import japanize_matplotlib\n",
        "from util import load_data, visualize_data_analysis, train_and_evaluate_model, get_insights_from_gemini, generate_insights, summarize_visualizations, get_visualization_insights\n",
        "\n",
        "st.title('アヤメの品種分類アプリ')\n",
        "st.write('このアプリはアヤメのデータセットを分析し、品種を予測します。')\n",
        "\n",
        "# APIキー入力部分\n",
        "api_key = st.text_input(\"APIキーを入力してください:\", value=\"\", type=\"password\")\n",
        "\n",
        "if api_key:\n",
        "    import google.generativeai as genai\n",
        "    genai.configure(api_key=api_key)\n",
        "\n",
        "# ファイルアップローダー\n",
        "train_file = st.file_uploader(\"訓練データをアップロードしてください（train.tsv）\", type=['tsv'])\n",
        "test_file = st.file_uploader(\"テストデータをアップロードしてください（test.tsv）\", type=['tsv'])\n",
        "\n",
        "# セッションステートを初期化\n",
        "if 'visualization_insights' not in st.session_state:\n",
        "    st.session_state.visualization_insights = None\n",
        "\n",
        "if train_file is not None and test_file is not None:\n",
        "    train_data = load_data(train_file)\n",
        "    test_data = load_data(test_file, drop_id=True)\n",
        "\n",
        "    # データの表示\n",
        "    st.write(\"訓練データの最初の5行:\")\n",
        "    st.write(train_data.head())\n",
        "    st.write(\"テストデータの最初の5行:\")\n",
        "    st.write(test_data.head())\n",
        "\n",
        "    # 基本統計と分布の表示\n",
        "    if st.checkbox('可視化によるデータ分析'):\n",
        "        visualize_data_analysis(train_data)\n",
        "\n",
        "        if st.button('Geminiに可視化結果を分析させる'):\n",
        "            visualization_summary = summarize_visualizations(train_data)\n",
        "            st.session_state.visualization_insights = get_visualization_insights(visualization_summary)\n",
        "            # st.write(\"Geminiからの知見:\")\n",
        "            # st.write(st.session_state.visualization_insights)\n",
        "\n",
        "    # セッションステートに保存されたGeminiからの知見を表示\n",
        "    if st.session_state.visualization_insights:\n",
        "        st.write(\"Geminiからの知見:\")\n",
        "        st.write(st.session_state.visualization_insights)\n",
        "\n",
        "    # モデルの訓練と評価\n",
        "    if st.button('モデルを訓練して評価'):\n",
        "        summary, y_test_pred = train_and_evaluate_model(train_data, test_data)\n",
        "        if summary:\n",
        "            st.write(\"モデルのパフォーマンス要約:\")\n",
        "            st.write(summary)\n",
        "\n",
        "            # Geminiを使って知見を取得\n",
        "            insights = get_insights_from_gemini(summary, y_test_pred)\n",
        "            st.write(\"Geminiからの知見:\")\n",
        "            st.write(insights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkqTpfBjlRi6",
        "outputId": "8c248305-9eec-4212-b597-2b4e853cdde8"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting streamlit_app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##アプリの起動"
      ],
      "metadata": {
        "id": "W4iAda4i1rxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ngrokを介してStreamlitを公開\n",
        "public_url = ngrok.connect(addr='8501')\n",
        "print('Public URL:', public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00aa9892-fa3d-4a25-ee0d-4be43d44f3b1",
        "id": "17c_nK0C1rxH"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://16dd-35-233-226-187.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streamlitアプリケーションの起動\n",
        "!streamlit run streamlit_app.py >/dev/null"
      ],
      "metadata": {
        "id": "RL66sp521rxI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b4e304e-9283-41dd-8d35-6538a0a73c82"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/util.py:33: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
            "  data.hist(bins=15, figsize=(15, 10), ax=ax)\n",
            "/content/util.py:33: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
            "  data.hist(bins=15, figsize=(15, 10), ax=ax)\n",
            "/content/util.py:33: UserWarning: To output multiple subplots, the figure containing the passed axes is being cleared.\n",
            "  data.hist(bins=15, figsize=(15, 10), ax=ax)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-05-14T05:27:08+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-ee649b7f-fce7-4165-8ec9-7dce5b4b2189 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2024-05-14T05:27:08+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-ee649b7f-fce7-4165-8ec9-7dce5b4b2189 err=\"failed to start tunnel: session closed\"\n"
          ]
        }
      ]
    }
  ]
}